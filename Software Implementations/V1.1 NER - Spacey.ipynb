{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3e49fe5",
   "metadata": {},
   "source": [
    "# Pipeline to view capabilities of Spacy Without Modification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceb9aafb",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fdb43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  Can I know wen r you gonna reply.  I called up ur cust care sum days bk n dey asked me 2 send an email 2 contact.in since it's related 2 offers.  I send dem an email n 2day dey ask me 2 contact sales team since dis was at da time of sales.FIFA fever is on in ur office!\n",
      "---- RT : .     \n",
      "I can't understand,Why so many issues with Dell La…\n",
      "---- A Win for America: Dell to Phase Out All Computer Chips Produced in China\n",
      "\n",
      "It’s about time. I hope this marks a trend. \n",
      "Thank you, ⁦⁩.  https://t.co/CZZwmQF9Bf\n",
      "----     I already connected with Dell reseller..Dell Service Center  and Dell care never told me, what is meaning of Denial of service. And what is reason. Every one told me contact to reseller. Now Reseller is helpless  Now what can I do for to repair\n",
      "----  Gotta love Dells team who don't listen to their customers. Supposed to receive a replacement monitor arm for the one damaged on arrival. You send a monitor, worse still it's the wrong monitor.\n",
      "----  i don't understand this mail from the company two days before talk charges https://t.co/JgtNaz8ACU no guarantee this laptop technically faulty not repaired so need to replace this laptop my son loss study so ASAP need new laptop need no 9833426158 https://t.co/fBYQmNt5eh\n",
      "----  Dear Dell,\n",
      "pls wake up and hv a lk at what you sales and customer team are doing. Shame on you guys for holding my money for 50dys &amp; making fool o/of myself. this shows hw is ur cstmr service &amp; wt priority that you guys give to cstmr.\n",
      "Rfnc Number:-158006885 #dellcustomercaresucks\n",
      "----  Your team on whatsapp named as Dell Technical Support are unable to resolve the issue that's why i am here\n",
      "----   Please do let me know if you get a reply\n",
      "---- RT :  The worst company ever u manufacturing ur laptop to broke after one year exactly inspiron 1454 2in1\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('../Datasets/new_twitter_data_men_rem_to_send.csv') as file:\n",
    "    dataset_csv_reader =  csv.reader(file)\n",
    "    \n",
    "    # Get headers\n",
    "    header = []\n",
    "    header = next(dataset_csv_reader)\n",
    "\n",
    "    #Extract tweets into list\n",
    "    tweets_list = []\n",
    "    for row in dataset_csv_reader:\n",
    "        tweets_list.append(row[1])\n",
    "    \n",
    "    #Print first 10 entries\n",
    "    for tweet in tweets_list[:10]:\n",
    "        print(f'---- {tweet}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a207394",
   "metadata": {},
   "source": [
    "## Run Spacey Small pipeline with Tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7934bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import spacy\n",
    "from itertools import groupby, zip_longest\n",
    "\n",
    "entities = {}\n",
    "TWEET_AMOUNT = 1000\n",
    "\n",
    "# load a pre-trained English language model from spacy library\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for doc in nlp.pipe(tweets_list[:TWEET_AMOUNT], disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']):\n",
    "    # Add all identified entities to a list, and store those under their label in the entities dictionary, no duplicates.\n",
    "    # e.g entities['ORG'] = ['Apple', 'Dell']\n",
    "    temp = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
    "    for key, value in temp.items():\n",
    "        if key in entities.keys():\n",
    "            entities[key] += value\n",
    "        else:\n",
    "            entities[key] = value\n",
    "\n",
    "entity_lists = [set(entities[key]) for key in entities.keys()]\n",
    "\n",
    "# Merge the lists of entities which are columns together, to create rows for the csv file\n",
    "entity_rows = zip_longest(*entity_lists, fillvalue=\" \")\n",
    "entity_rows = [*entity_rows]\n",
    "\n",
    "# Write list of recognised entities to a csv file in columns under their label\n",
    "with open('entities_recognised(en_core_web_sm).csv','w') as out:\n",
    "    csv_out=csv.writer(out, delimiter=',', skipinitialspace=True)\n",
    "    csv_out.writerow([key for key in entities.keys()])\n",
    "    for row in entity_rows:\n",
    "        csv_out.writerow(row)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63800715",
   "metadata": {},
   "source": [
    "## Run Spacey Accuracy Pipeline with Tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4515019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import spacy\n",
    "from itertools import groupby, zip_longest\n",
    "\n",
    "entities = {}\n",
    "TWEET_AMOUNT = 1000\n",
    "\n",
    "# load a pre-trained English language model from spacy library\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "for doc in nlp.pipe(tweets_list[:TWEET_AMOUNT], disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']):\n",
    "    # Add all identified entities to a list, and store those under their label in the entities dictionary, no duplicates.\n",
    "    # e.g entities['ORG'] = ['Apple', 'Dell']\n",
    "    temp = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
    "    for key, value in temp.items():\n",
    "        if key in entities.keys():\n",
    "            entities[key] += value\n",
    "        else:\n",
    "            entities[key] = value\n",
    "\n",
    "entity_lists = [set(entities[key]) for key in entities.keys()]\n",
    "\n",
    "# Merge the lists of entities which are columns, together, to create rows for the csv file\n",
    "entity_rows = zip_longest(*entity_lists, fillvalue=\" \")\n",
    "entity_rows = [*entity_rows]\n",
    "\n",
    "# Write list of recognised entities to a csv file in columns under their label\n",
    "with open('entities_recognised(en_core_web_trf).csv','w') as out:\n",
    "    csv_out=csv.writer(out, delimiter=',', skipinitialspace=True)\n",
    "    csv_out.writerow([key for key in entities.keys()])\n",
    "    for row in entity_rows:\n",
    "        csv_out.writerow(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "11a451f67fa69ea0d8117e50b3c0eec8c61097f5acea211e960b0ebee8530c9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
