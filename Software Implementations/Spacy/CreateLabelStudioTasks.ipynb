{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  Can I know wen r you gonna reply.  I called up ur cust care sum days bk n dey asked me 2 send an email 2 contact.in since it's related 2 offers.  I send dem an email n 2day dey ask me 2 contact sales team since dis was at da time of sales.FIFA fever is on in ur office!\n",
      "---- RT : .     \n",
      "I can't understand,Why so many issues with Dell La…\n",
      "---- A Win for America: Dell to Phase Out All Computer Chips Produced in China\n",
      "\n",
      "It’s about time. I hope this marks a trend. \n",
      "Thank you, ⁦⁩.  https://t.co/CZZwmQF9Bf\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "tweets_list = []\n",
    "\n",
    "with open('../Datasets/new_twitter_data_men_rem_to_send.csv') as file:\n",
    "    dataset_csv_reader =  csv.reader(file)\n",
    "    \n",
    "    # Get headers\n",
    "    header = []\n",
    "    header = next(dataset_csv_reader)\n",
    "\n",
    "    #Extract tweets into list\n",
    "    tweets_list = []\n",
    "    for row in dataset_csv_reader:\n",
    "        tweets_list.append(row[1])\n",
    "    \n",
    "    #Print first 3 entries\n",
    "    for tweet in tweets_list[:3]:\n",
    "        print(f'---- {tweet}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the text through Spacy Accuracy Model and Convert To Label Studio Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save 2857 tasks to \"tasks.json\"\n",
      "Named entities are saved to \"named_entities.txt\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from itertools import groupby\n",
    "import json\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# This function converts spaCy docs to the list of named entity spans in Label Studio compatible JSON format:\n",
    "def doc_to_spans(doc):\n",
    "    tokens = [(tok.text, tok.idx, tok.ent_type_) for tok in doc]\n",
    "    results = []\n",
    "    entities = set()\n",
    "    for entity, group in groupby(tokens, key=lambda t: t[-1]):\n",
    "        if not entity:\n",
    "            continue\n",
    "        group = list(group)\n",
    "        _, start, _ = group[0]\n",
    "        word, last, _ = group[-1]\n",
    "        text = ' '.join(item[0] for item in group)\n",
    "        end = last + len(word)\n",
    "        results.append({\n",
    "            'from_name': 'label',\n",
    "            'to_name': 'text',\n",
    "            'type': 'labels',\n",
    "            'value': {\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'text': text,\n",
    "                'labels': [entity]\n",
    "            }\n",
    "        })\n",
    "        entities.add(entity)\n",
    "\n",
    "    return results, entities\n",
    "\n",
    "# Prepare Label Studio tasks in import JSON format with the model predictions:\n",
    "entities = set()\n",
    "tasks = []\n",
    "for text in tweets_list:\n",
    "    predictions = []\n",
    "    doc = nlp(text)\n",
    "    spans, ents = doc_to_spans(doc)\n",
    "    entities |= ents\n",
    "    predictions.append({'model_version': 'en_core_web_lg', 'result': spans})\n",
    "    tasks.append({\n",
    "        'data': {'text': text},\n",
    "        'predictions': predictions\n",
    "    })\n",
    "\n",
    "# Save Label Studio tasks.json\n",
    "print(f'Save {len(tasks)} tasks to \"tasks.json\"')\n",
    "with open('twitter_tasks.json', mode='w') as f:\n",
    "    json.dump(tasks, f, indent=2)\n",
    "    \n",
    "# Save class labels as a txt file\n",
    "print('Named entities are saved to \"named_entities.txt\"')\n",
    "with open('named_entities_twitter.txt', mode='w') as f:\n",
    "    f.write('\\n'.join(sorted(entities)))\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Lock Kensigton for XPS\n",
      "---- Hello! What do you think about it? I would like to buy this lock for when I use my laptop on the go. Does it have any contraindications? Do you think it can easily be folded for carrying in a backpack?\n",
      "\n",
      "Or it would be better for transport to have a lock with a spiral wire (but I can't find compatibility with XPS 9720) ...\n",
      "\n",
      "[https://www.dell.com/en-us/shop/slim-n17-20-keyed-dual-head-laptop-lock-for-wedge-shaped-slots/apd/ab868415/pc-accessories#techspecs\\_section](https://www.dell.com/en-us/shop/slim-n17-20-keyed-dual-head-laptop-lock-for-wedge-shaped-slots/apd/ab868415/pc-accessories#techspecs_section)\n",
      "\n",
      "Thank you very much.\n",
      "\n",
      "Edit:  \\*Kensington\n",
      "\n",
      "Edit: do you know if these products are also shipped from China, or is there a stock in Europe?\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "https://preview.redd.it/e48fjc3yatx91.png?width=3317&format=png&auto=webp&v=enabled&s=6c2253497e182c3c6f9b299ba06b382a5ea50ae6\n",
      "\n",
      "&#x200B;\n",
      "---- Dell Latitude 5490 suggestions for LCD upgrade\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('../../Datasets/dell_output_tcd_processed.csv') as file:\n",
    "    dataset_csv_reader =  csv.reader(file)\n",
    "    \n",
    "    # Get headers\n",
    "    header = []\n",
    "    header = next(dataset_csv_reader)\n",
    "\n",
    "    #Extract tweets into list\n",
    "    text_list = []\n",
    "    for row in dataset_csv_reader:\n",
    "        text_list.append(row[0])\n",
    "    \n",
    "    #Print first 3 entries\n",
    "    for text in text_list[:3]:\n",
    "        print(f'---- {text}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the text through Spacy Accuracy Model and Convert To Label Studio Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save 4179 tasks to \"reddit_tasks.json\"\n",
      "Named entities are saved to \"named_entities_reddit.txt\"\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from itertools import groupby\n",
    "import json\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# This function converts spaCy docs to the list of named entity spans in Label Studio compatible JSON format:\n",
    "def doc_to_spans(doc):\n",
    "    tokens = [(tok.text, tok.idx, tok.ent_type_) for tok in doc]\n",
    "    results = []\n",
    "    entities = set()\n",
    "    for entity, group in groupby(tokens, key=lambda t: t[-1]):\n",
    "        if not entity:\n",
    "            continue\n",
    "        group = list(group)\n",
    "        _, start, _ = group[0]\n",
    "        word, last, _ = group[-1]\n",
    "        text = ' '.join(item[0] for item in group)\n",
    "        end = last + len(word)\n",
    "        results.append({\n",
    "            'from_name': 'label',\n",
    "            'to_name': 'text',\n",
    "            'type': 'labels',\n",
    "            'value': {\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'text': text,\n",
    "                'labels': [entity]\n",
    "            }\n",
    "        })\n",
    "        entities.add(entity)\n",
    "\n",
    "    return results, entities\n",
    "\n",
    "# Prepare Label Studio tasks in import JSON format with the model predictions:\n",
    "entities = set()\n",
    "tasks = []\n",
    "for text in text_list:\n",
    "    predictions = []\n",
    "    doc = nlp(text)\n",
    "    spans, ents = doc_to_spans(doc)\n",
    "    entities |= ents\n",
    "    predictions.append({'model_version': 'en_core_web_trf', 'result': spans})\n",
    "    tasks.append({\n",
    "        'data': {'text': text},\n",
    "        'predictions': predictions\n",
    "    })\n",
    "\n",
    "# Save Label Studio tasks.json\n",
    "print(f'Save {len(tasks)} tasks to \"reddit_tasks.json\"')\n",
    "with open('reddit_tasks.json', mode='w') as f:\n",
    "    json.dump(tasks, f, indent=2)\n",
    "    \n",
    "# Save class labels as a txt file\n",
    "print('Named entities are saved to \"named_entities_reddit.txt\"')\n",
    "with open('named_entities_reddit.txt', mode='w') as f:\n",
    "    f.write('\\n'.join(sorted(entities)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1197922063c724986158a3dd3f92ff66deafc1eeaeb64f89e94ea3a83b139bff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
