{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3e49fe5",
   "metadata": {},
   "source": [
    "# Pipeline to view capabilities of Spacy Without Modification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceb9aafb",
   "metadata": {},
   "source": [
    "## Load the Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90fdb43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----  Can I know wen r you gonna reply.  I called up ur cust care sum days bk n dey asked me 2 send an email 2 contact.in since it's related 2 offers.  I send dem an email n 2day dey ask me 2 contact sales team since dis was at da time of sales.FIFA fever is on in ur office!\n",
      "---- RT : .     \n",
      "I can't understand,Why so many issues with Dell La…\n",
      "---- A Win for America: Dell to Phase Out All Computer Chips Produced in China\n",
      "\n",
      "It’s about time. I hope this marks a trend. \n",
      "Thank you, ⁦⁩.  https://t.co/CZZwmQF9Bf\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "with open('../Datasets/new_twitter_data_men_rem_to_send.csv') as file:\n",
    "    dataset_csv_reader =  csv.reader(file)\n",
    "    \n",
    "    # Get headers\n",
    "    header = []\n",
    "    header = next(dataset_csv_reader)\n",
    "\n",
    "    #Extract tweets into list\n",
    "    tweets_list = []\n",
    "    for row in dataset_csv_reader:\n",
    "        tweets_list.append(row[1])\n",
    "    \n",
    "    #Print first 3 entries\n",
    "    for tweet in tweets_list[:3]:\n",
    "        print(f'---- {tweet}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a207394",
   "metadata": {},
   "source": [
    "## Run Spacey Small pipeline with Tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7934bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import spacy\n",
    "from itertools import groupby, zip_longest\n",
    "\n",
    "entities = {}\n",
    "TWEET_AMOUNT = 1000\n",
    "\n",
    "# load a pre-trained English language model from spacy library\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for doc in nlp.pipe(tweets_list[:TWEET_AMOUNT], disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']):\n",
    "    # Add all identified entities to a list, and store those under their label in the entities dictionary, no duplicates.\n",
    "    # e.g entities['ORG'] = ['Apple', 'Dell']\n",
    "    temp = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
    "    for key, value in temp.items():\n",
    "        if key in entities.keys():\n",
    "            entities[key] += value\n",
    "        else:\n",
    "            entities[key] = value\n",
    "\n",
    "entity_lists = [set(entities[key]) for key in entities.keys()]\n",
    "\n",
    "# Merge the lists of entities which are columns together, to create rows for the csv file\n",
    "entity_rows = zip_longest(*entity_lists, fillvalue=\" \")\n",
    "entity_rows = [*entity_rows]\n",
    "\n",
    "# Write list of recognised entities to a csv file in columns under their label\n",
    "with open('entities_recognised(en_core_web_sm).csv','w') as out:\n",
    "    csv_out=csv.writer(out, delimiter=',', skipinitialspace=True)\n",
    "    csv_out.writerow([key for key in entities.keys()])\n",
    "    for row in entity_rows:\n",
    "        csv_out.writerow(row)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63800715",
   "metadata": {},
   "source": [
    "## Run Spacey Accuracy Pipeline with Tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4515019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import spacy\n",
    "from itertools import groupby, zip_longest\n",
    "\n",
    "entities = {}\n",
    "TWEET_AMOUNT = 1000\n",
    "\n",
    "# load a pre-trained English language model from spacy library\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "for doc in nlp.pipe(tweets_list[:TWEET_AMOUNT], disable=['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']):\n",
    "    # Add all identified entities to a list, and store those under their label in the entities dictionary, no duplicates.\n",
    "    # e.g entities['ORG'] = ['Apple', 'Dell']\n",
    "    temp = {key: list(set(map(lambda x: str(x), g))) for key, g in groupby(sorted(doc.ents, key=lambda x: x.label_), lambda x: x.label_)}\n",
    "    for key, value in temp.items():\n",
    "        if key in entities.keys():\n",
    "            entities[key] += value\n",
    "        else:\n",
    "            entities[key] = value\n",
    "\n",
    "entity_lists = [set(entities[key]) for key in entities.keys()]\n",
    "\n",
    "# Merge the lists of entities which are columns, together, to create rows for the csv file\n",
    "entity_rows = zip_longest(*entity_lists, fillvalue=\" \")\n",
    "entity_rows = [*entity_rows]\n",
    "\n",
    "# Write list of recognised entities to a csv file in columns under their label\n",
    "with open('entities_recognised(en_core_web_trf).csv','w') as out:\n",
    "    csv_out=csv.writer(out, delimiter=',', skipinitialspace=True)\n",
    "    csv_out.writerow([key for key in entities.keys()])\n",
    "    for row in entity_rows:\n",
    "        csv_out.writerow(row)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0916abd0",
   "metadata": {},
   "source": [
    "# Load the reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4982cb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "11a451f67fa69ea0d8117e50b3c0eec8c61097f5acea211e960b0ebee8530c9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
